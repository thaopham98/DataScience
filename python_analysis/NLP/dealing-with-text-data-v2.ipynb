{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"0\"></a>\n",
    "# **Dealing with Text Data**\n",
    "\n",
    "\n",
    "Hello class,\n",
    "\n",
    "\n",
    "Let's observe and discuss several common ways to deal with text data. We will discuss different feature extraction methods. We will start with some basic techniques which will lead into advanced Natural Language Processing techniques. We will also learn about pre-processing of the text data in order to extract better features from clean data.\n",
    "\n",
    "So, let's dive in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"0.1\"></a>\n",
    "# **Table of Contents** \n",
    "\n",
    "1.\t[Introduction to Natural Language Processing](#1)\n",
    "2.\t[Basic feature extraction using text data](#2)\n",
    "   - 2.1 [Count number of words](#2.1)\n",
    "   - 2.2 [Count number of characters](#2.2)\n",
    "   - 2.3 [Average word length](#2.3)\n",
    "   - 2.4 [Number of stopwords](#2.4)\n",
    "   - 2.5 [Number of special characters](#2.5)\n",
    "   - 2.6 [Number of numerics](#2.6)\n",
    "   - 2.7 [Number of uppercase words](#2.7)\n",
    "3.\t[Basic Text Pre-processing of text data](#3)\n",
    "   - 3.1 [CountVectorization](#3.1)\n",
    "   - 3.2 [HashingVectorizer](#3.2)\n",
    "   - 3.3 [Lower casing](#3.3)\n",
    "   - 3.4 [Punctuation removal](#3.4)\n",
    "   - 3.5 [Stopwords removal](#3.5)\n",
    "   - 3.6 [Frequent words removal](#3.6)\n",
    "   - 3.7 [Rare words removal](#3.7)\n",
    "   - 3.8 [Spelling correction](#3.8)\n",
    "   - 3.9 [Tokenization](#3.9)\n",
    "   - 3.10 [Stemming](#3.10)\n",
    "   - 3.11 [Lemmatization](#3.11)\n",
    "4.\t[Advance Text Processing](#4)\n",
    "   - 4.1 [N-grams](#4.1)\n",
    "   - 4.2 [Term Frequency](#4.2)\n",
    "   - 4.3 [Inverse Document Frequency](#4.3)\n",
    "   - 4.4 [Term Frequency-Inverse Document Frequency (TF-IDF)](#4.4)\n",
    "   - 4.5 [Bag of Words](#4.5)\n",
    "   - 4.6 [Sentiment Analysis](#4.6)\n",
    "5.  [References](#5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **1. Introduction to Natural Language Processing** <a class=\"anchor\" id=\"1\"></a>\n",
    "\n",
    "\n",
    "[Table of Contents](#0.1)\n",
    "\n",
    "\n",
    "- In a data scientist's journey, we will often come across a term **Natural Language Processing (NLP)**. But, have you ever wondered what do we mean by Natural Language Processing. In short, NLP is the art of understanding the meaning and influence of words.\n",
    "\n",
    "\n",
    "- In terms of wikipedia - \n",
    "\n",
    "\n",
    "[Natural Language Processing](https://en.wikipedia.org/wiki/Natural_language_processing) is a subfield of linguistics, computer science, information engineering, and artificial intelligence concerned with the interactions between computers and human (natural) languages, in particular how to program computers to process and analyze large amounts of natural language data.\n",
    "\n",
    "Challenges in natural language processing frequently involve [speech recognition](https://en.wikipedia.org/wiki/Speech_recognition), [natural language understanding](https://en.wikipedia.org/wiki/Natural-language_understanding) and [natural language generation](https://en.wikipedia.org/wiki/Natural-language_generation)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "Some of the most useful NLP techniques currently used in practice are:-\n",
    "\n",
    "- **CountVectorization**, \n",
    "- **Hashing Vectorization**, \n",
    "- **Term Frequency-Inverse Document Frequency (TF-IDF)**, \n",
    "- **Lemmatization**, \n",
    "- **Stemming**, \n",
    "- **Parsing**, and \n",
    "- **Sentiment Analysis**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **2. Basic feature extraction using text data** <a class=\"anchor\" id=\"2\"></a>\n",
    "\n",
    "\n",
    "[Table of Contents](#0.1)\n",
    "\n",
    "\n",
    "- This assumes you don't have sufficient knowledge of Natural Language Processing.\n",
    "\n",
    "- We can still use some basic feature extraction techniques to extract features from text data.\n",
    "\n",
    "- We will discuss some basic feature extraction techniques.\n",
    "\n",
    "- But, first let's import the data. (Please check canvas and download data from the data folder called nlp_train.csv and nlp_test.csv)\n",
    "\n",
    "- This data is from Twitter and contains user comments\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Load necessary libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Read dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('nlp_train.csv')\n",
    "test = pd.read_csv('nlp_test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Preview dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>@user when a father is dysfunctional and is s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>@user @user thanks for #lyft credit i can't us...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>bihday your majesty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>#model   i love u take with u all the time in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>factsguide: society now    #motivation</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  label                                              tweet\n",
       "0   1      0   @user when a father is dysfunctional and is s...\n",
       "1   2      0  @user @user thanks for #lyft credit i can't us...\n",
       "2   3      0                                bihday your majesty\n",
       "3   4      0  #model   i love u take with u all the time in ...\n",
       "4   5      0             factsguide: society now    #motivation"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>31963</td>\n",
       "      <td>#studiolife #aislife #requires #passion #dedic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>31964</td>\n",
       "      <td>@user #white #supremacists want everyone to s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>31965</td>\n",
       "      <td>safe ways to heal your #acne!!    #altwaystohe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>31966</td>\n",
       "      <td>is the hp and the cursed child book up for res...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>31967</td>\n",
       "      <td>3rd #bihday to my amazing, hilarious #nephew...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                              tweet\n",
       "0  31963  #studiolife #aislife #requires #passion #dedic...\n",
       "1  31964   @user #white #supremacists want everyone to s...\n",
       "2  31965  safe ways to heal your #acne!!    #altwaystohe...\n",
       "3  31966  is the hp and the cursed child book up for res...\n",
       "4  31967    3rd #bihday to my amazing, hilarious #nephew..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2.1 Count number of words** <a class=\"anchor\" id=\"2.1\"></a>\n",
    "\n",
    "\n",
    "[Table of Contents](#0.1)\n",
    "\n",
    "\n",
    "- One of the most basic requirement in NLP analysis is to count the number of words in each tweet. The idea behind this is that **the negative sentiments contain a lesser amount of words than the positive ones**.\n",
    "\n",
    "- We can accomplish the above task (count the number of words) by using the **split** function in python as follows-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_of_words(df):\n",
    "    df['word_count'] = df['tweet'].apply(lambda x : len(str(x).split(\" \"))) # word_count is now a new feature will be added to the dataset. this is part of Feature Extraction\n",
    "    print(df[['tweet','word_count']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               tweet  word_count\n",
      "0   @user when a father is dysfunctional and is s...          21\n",
      "1  @user @user thanks for #lyft credit i can't us...          22\n",
      "2                                bihday your majesty           5\n",
      "3  #model   i love u take with u all the time in ...          17\n",
      "4             factsguide: society now    #motivation           8\n"
     ]
    }
   ],
   "source": [
    "num_of_words(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               tweet  word_count\n",
      "0  #studiolife #aislife #requires #passion #dedic...          12\n",
      "1   @user #white #supremacists want everyone to s...          20\n",
      "2  safe ways to heal your #acne!!    #altwaystohe...          15\n",
      "3  is the hp and the cursed child book up for res...          24\n",
      "4    3rd #bihday to my amazing, hilarious #nephew...          18\n"
     ]
    }
   ],
   "source": [
    "num_of_words(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can see that word counts in every tweet has been calculated above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  **2.2 Count number of characters**  <a class=\"anchor\" id=\"2.2\"></a>\n",
    "\n",
    "\n",
    "[Table of Contents](#0.1)\n",
    "\n",
    "\n",
    "- We can also calculate the number of characters in every tweet. The intuition is same as above.\n",
    "\n",
    "- This can be accomplised by calculating the length of the tweet as follows -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_of_chars(df):\n",
    "    df['char_count'] = df['tweet'].str.len() ## this also includes spaces\n",
    "    print(df[['tweet','char_count']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               tweet  char_count\n",
      "0   @user when a father is dysfunctional and is s...         102\n",
      "1  @user @user thanks for #lyft credit i can't us...         122\n",
      "2                                bihday your majesty          21\n",
      "3  #model   i love u take with u all the time in ...          86\n",
      "4             factsguide: society now    #motivation          39\n"
     ]
    }
   ],
   "source": [
    "num_of_chars(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               tweet  char_count\n",
      "0  #studiolife #aislife #requires #passion #dedic...          90\n",
      "1   @user #white #supremacists want everyone to s...         101\n",
      "2  safe ways to heal your #acne!!    #altwaystohe...          71\n",
      "3  is the hp and the cursed child book up for res...         142\n",
      "4    3rd #bihday to my amazing, hilarious #nephew...          93\n"
     ]
    }
   ],
   "source": [
    "num_of_chars(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can see that character counts in every tweet has been calculated above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2.3 Average word length** <a class=\"anchor\" id=\"2.3\"></a>\n",
    "\n",
    "\n",
    "[Table of Contents](#0.1)\n",
    "\n",
    "\n",
    "- Now, number of words and number of characters are important. But, there is another feature which is also important is **average word length** of each tweet. This feature can help us to improve our model.\n",
    "\n",
    "- We can accomplish the above task by simply taking the sum of the length of all the words and divide it by the total length of the tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_word(sentence):\n",
    "    words = sentence.split()    \n",
    "    return (sum(len(word) for word in words)/len(words)) # Add them all up and divide by number that there are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_word_length(df): # Now that we have a function that performs the operation, apply to all tweets\n",
    "    df['avg_word'] = df['tweet'].apply(lambda x: avg_word(x))\n",
    "    print(df[['tweet','avg_word']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               tweet  avg_word\n",
      "0   @user when a father is dysfunctional and is s...  4.555556\n",
      "1  @user @user thanks for #lyft credit i can't us...  5.315789\n",
      "2                                bihday your majesty  5.666667\n",
      "3  #model   i love u take with u all the time in ...  4.928571\n",
      "4             factsguide: society now    #motivation  8.000000\n"
     ]
    }
   ],
   "source": [
    "avg_word_length(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               tweet  avg_word\n",
      "0  #studiolife #aislife #requires #passion #dedic...  8.777778\n",
      "1   @user #white #supremacists want everyone to s...  5.125000\n",
      "2  safe ways to heal your #acne!!    #altwaystohe...  6.333333\n",
      "3  is the hp and the cursed child book up for res...  5.409091\n",
      "4    3rd #bihday to my amazing, hilarious #nephew...  5.066667\n"
     ]
    }
   ],
   "source": [
    "avg_word_length(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2.4 Number of stopwords** <a class=\"anchor\" id=\"2.4\"></a>\n",
    "\n",
    "\n",
    "[Table of Contents](#0.1)\n",
    "\n",
    "\n",
    "- Generally, while solving any NLP problem, the first thing we do is to remove the stopwords. DO you remember what are stop words?\n",
    "\n",
    "- **Stop Words** : A stop word is a commonly used word such as `the`, `a`, `an`, `in` which are filtered out before or after processing of natural language data (text). Sometimes calculating the number of stopwords can also give us some extra information which we might have been losing before.\n",
    "\n",
    "- For more information on stop words, please visit the following links-\n",
    "\n",
    "- https://en.wikipedia.org/wiki/Stop_words\n",
    "\n",
    "- https://www.geeksforgeeks.org/removing-stop-words-nltk-python/\n",
    "\n",
    "- https://kavita-ganesan.com/what-are-stop-words/#.XowrncgzbIU\n",
    "\n",
    "\n",
    "- To check the list of stopwords we can type the following commands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Using cached nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: click in /opt/anaconda3/envs/DataScienceStudy/lib/python3.12/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /opt/anaconda3/envs/DataScienceStudy/lib/python3.12/site-packages (from nltk) (1.4.2)\n",
      "Collecting regex>=2021.8.3 (from nltk)\n",
      "  Using cached regex-2024.11.6-cp312-cp312-macosx_10_13_x86_64.whl.metadata (40 kB)\n",
      "Collecting tqdm (from nltk)\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Using cached nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "Using cached regex-2024.11.6-cp312-cp312-macosx_10_13_x86_64.whl (288 kB)\n",
      "Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Installing collected packages: tqdm, regex, nltk\n",
      "Successfully installed nltk-3.9.1 regex-2024.11.6 tqdm-4.67.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/thaophamsmacbook/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = stopwords.words('english') # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can count the number of stopwords as follows-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stop_words(df):\n",
    "    df['stopwords'] = df['tweet'].apply(lambda x: len([x for x in x.split() if x in stop]))\n",
    "    print(df[['tweet','stopwords']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               tweet  stopwords\n",
      "0   @user when a father is dysfunctional and is s...         10\n",
      "1  @user @user thanks for #lyft credit i can't us...          5\n",
      "2                                bihday your majesty          1\n",
      "3  #model   i love u take with u all the time in ...          5\n",
      "4             factsguide: society now    #motivation          1\n"
     ]
    }
   ],
   "source": [
    "stop_words(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               tweet  stopwords\n",
      "0  #studiolife #aislife #requires #passion #dedic...          1\n",
      "1   @user #white #supremacists want everyone to s...          4\n",
      "2  safe ways to heal your #acne!!    #altwaystohe...          2\n",
      "3  is the hp and the cursed child book up for res...          8\n",
      "4    3rd #bihday to my amazing, hilarious #nephew...          4\n"
     ]
    }
   ],
   "source": [
    "stop_words(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2.5 Number of special characters** <a class=\"anchor\" id=\"2.5\"></a>\n",
    "\n",
    "\n",
    "[Table of Contents](#0.1)\n",
    "\n",
    "\n",
    "- One more interesting feature which we can extract from a tweet is to calculate the number of hashtags in it. It also helps in extracting extra information from our text data.\n",
    "\n",
    "- Here, we make use of the `starts with` function because hashtags always appear at the beginning of a word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hash_tags(df):\n",
    "    df['hashtags'] = df['tweet'].apply(lambda x: len([x for x in x.split() if x.startswith('#')]))\n",
    "    print(df[['tweet','hashtags']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               tweet  hashtags\n",
      "0   @user when a father is dysfunctional and is s...         1\n",
      "1  @user @user thanks for #lyft credit i can't us...         3\n",
      "2                                bihday your majesty         0\n",
      "3  #model   i love u take with u all the time in ...         1\n",
      "4             factsguide: society now    #motivation         1\n"
     ]
    }
   ],
   "source": [
    "hash_tags(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               tweet  hashtags\n",
      "0  #studiolife #aislife #requires #passion #dedic...         7\n",
      "1   @user #white #supremacists want everyone to s...         4\n",
      "2  safe ways to heal your #acne!!    #altwaystohe...         4\n",
      "3  is the hp and the cursed child book up for res...         3\n",
      "4    3rd #bihday to my amazing, hilarious #nephew...         2\n"
     ]
    }
   ],
   "source": [
    "hash_tags(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2.6 Number of numerics** <a class=\"anchor\" id=\"2.6\"></a>\n",
    "\n",
    "\n",
    "[Table of Contents](#0.1)\n",
    "\n",
    "\n",
    "- Just like we calculated the number of words, we can also calculate the number of numerics which are present in the tweets. It is a useful feature that should be run while doing similar exercises. For example - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_numerics(df):\n",
    "    df['numerics'] = df['tweet'].apply(lambda x: len([x for x in x.split() if x.isdigit()]))\n",
    "    print(df[['tweet','numerics']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               tweet  numerics\n",
      "0   @user when a father is dysfunctional and is s...         0\n",
      "1  @user @user thanks for #lyft credit i can't us...         0\n",
      "2                                bihday your majesty         0\n",
      "3  #model   i love u take with u all the time in ...         0\n",
      "4             factsguide: society now    #motivation         0\n"
     ]
    }
   ],
   "source": [
    "num_numerics(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               tweet  numerics\n",
      "0  #studiolife #aislife #requires #passion #dedic...         0\n",
      "1   @user #white #supremacists want everyone to s...         0\n",
      "2  safe ways to heal your #acne!!    #altwaystohe...         0\n",
      "3  is the hp and the cursed child book up for res...         0\n",
      "4    3rd #bihday to my amazing, hilarious #nephew...         0\n"
     ]
    }
   ],
   "source": [
    "num_numerics(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.7 Number of Uppercase words  \n",
    "<a class=\"anchor\" id=\"2.7\"></a>\n",
    "\n",
    "\n",
    "[Table of Contents](#0.1)\n",
    "\n",
    "\n",
    "- Anger or rage is quite often expressed by writing in UPPERCASE words which makes this a necessary operation to identify those words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_uppercase(df):\n",
    "    df['upper_case'] = df['tweet'].apply(lambda x: len([x for x in x.split() if x.isupper()]))\n",
    "    print(df[['tweet','upper_case']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               tweet  upper_case\n",
      "0   @user when a father is dysfunctional and is s...           0\n",
      "1  @user @user thanks for #lyft credit i can't us...           0\n",
      "2                                bihday your majesty           0\n",
      "3  #model   i love u take with u all the time in ...           0\n",
      "4             factsguide: society now    #motivation           0\n"
     ]
    }
   ],
   "source": [
    "num_uppercase(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               tweet  upper_case\n",
      "0  #studiolife #aislife #requires #passion #dedic...           0\n",
      "1   @user #white #supremacists want everyone to s...           0\n",
      "2  safe ways to heal your #acne!!    #altwaystohe...           0\n",
      "3  is the hp and the cursed child book up for res...           0\n",
      "4    3rd #bihday to my amazing, hilarious #nephew...           0\n"
     ]
    }
   ],
   "source": [
    "num_uppercase(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **3. Basic Text Pre-Processing** <a class=\"anchor\" id=\"3\"></a>\n",
    "\n",
    "\n",
    "[Table of Contents](#0.1)\n",
    "\n",
    "\n",
    "- So far, we have learned how to extract basic features from text data. \n",
    "\n",
    "- Now, we move onto text and feature extraction. \n",
    "\n",
    "- Our first step should be to clean the data in order to obtain better features. \n",
    "\n",
    "- We will achieve this by doing some of the basic pre-processing steps on our training data.\n",
    "\n",
    "- So, let’s get into it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3.1 CountVectorization** <a class=\"anchor\" id=\"3.1\"></a>\n",
    "\n",
    "\n",
    "[Table of Contents](#0.1)\n",
    "\n",
    "\n",
    "- **[CounterVectorization](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)** is a SciKitLearn library takes any text document and returns each unique word as a feature with the count of number of times that word occurs. In english, here is a text, count unique words in the text.\n",
    "\n",
    "- While this can generate lot of features with some extremely useful parameters that help avoid that including stop_words, n_grams, and max_features. \n",
    "\n",
    "- **Stop words** generates a list of words that will not be included as a feature. The primary use of this is the “English” dictionary where it will get rid of insignificant words like “is, the, a, it, as “which can appear quite frequently, but have little to no influence on our end goal. \n",
    "\n",
    "- **ngrams_range** selects how you can group words together. Instead of having NLP return each word separately, we can get results like “Hello again” if it equals 2 or “See you later” if it equals 3. \n",
    "\n",
    "- **max_features** is how many features you choose to create. If we choose it to equal none it means that we will get every and all words as features, but if we set it equal to 50 you will only get the 50 most frequently used words.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of ['and' 'document' 'first' 'is' 'one' 'second' 'the' 'third' 'this'] is 9\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "corpus = [\n",
    "          'This is the first document.',\n",
    "          'This document is the second document.',\n",
    "          'And this is the third one.',\n",
    "          'Is this the first document?',\n",
    "         ]\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "X1 = vectorizer.fit_transform(corpus)\n",
    "print(f'Length of {vectorizer.get_feature_names_out()} is {len(vectorizer.get_feature_names_out())}') # This is returning each unique word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 1 1 0 0 1 0 1]\n",
      " [0 2 0 1 0 1 1 0 1]\n",
      " [1 0 0 1 1 0 1 1 1]\n",
      " [0 1 1 1 0 0 1 0 1]]\n"
     ]
    }
   ],
   "source": [
    "print(X1.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at this, try to do the first one your self. IF given the first line in the corpus, what vector would you produce using the `vectorizer.get_feature_names_out()`.<br>\n",
    "`ngram_range`: a sequence of a specified number of items, such as words, numbers, symbols, or punctuation, that appear in a given order in a text document or speech recording"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 1 1 0 0 1 0 0 0 0 1 0]\n",
      " [0 1 0 1 0 1 0 1 0 0 1 0 0]\n",
      " [1 0 0 1 0 0 0 0 1 1 0 1 0]\n",
      " [0 0 1 0 1 0 1 0 0 0 0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "vectorizer2 = CountVectorizer(analyzer='word', ngram_range=(2, 2)) \n",
    "# Whether the feature should be made of word n-gram or character n-grams\n",
    "# The lower and upper boundary of the range of n-values for different word n-grams or char n-grams to be extracted.\n",
    "X2 = vectorizer2.fit_transform(corpus)\n",
    "# Learn the vocabulary dictionary and return document-term matrix.\n",
    "\n",
    "# This is equivalent to fit followed by transform, but more efficiently implemented.\n",
    "print(X2.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is what it used to create the above vectors. Again use the first line in the corpus to see what vecotr it produces. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['and this' 'document is' 'first document' 'is the' 'is this'\n",
      " 'second document' 'the first' 'the second' 'the third' 'third one'\n",
      " 'this document' 'this is' 'this the']\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer2.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3.2 HashingVectorizer** <a class=\"anchor\" id=\"3.2\"></a>\n",
    "\n",
    "\n",
    "[Table of Contents](#0.1)\n",
    "\n",
    "\n",
    "- [Hashing Vectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.HashingVectorizer.html) converts text to a matrix of occurrences using the “hashing trick”.\n",
    "\n",
    "\n",
    "- It converts a collection of text documents to a matrix of token occurrences.\n",
    "\n",
    "- It turns a collection of text documents into a scipy.sparse matrix holding token occurrence counts (or binary occurrence information), possibly normalized as token frequencies if norm=’l1’ or projected on the euclidean unit sphere if norm=’l2’.\n",
    "\n",
    "- This text vectorizer implementation uses the hashing trick to find the token string name to feature integer index mapping.\n",
    "\n",
    "- Each word is mapped to a feature and using the hash function converts it to a hash. \n",
    "\n",
    "- If the word occurs again in the body of the text it is converted to that same feature which allows us to count it in the same feature without retaining a dictionary in memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 16)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "corpus = [\n",
    "          'This is the first document.',\n",
    "          'This document is the second document.',\n",
    "          'And this is the third one.',\n",
    "          'Is this the first document?',\n",
    "         ]\n",
    "vectorizer = HashingVectorizer(n_features=16, norm=None) # where did the n_featuures == 16 come from?\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s print the columns that are populated for the first document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Compressed Sparse Row sparse matrix of dtype 'float64'\n",
      "\twith 4 stored elements and shape (1, 16)>\n",
      "  Coords\tValues\n",
      "  (0, 0)\t-1.0\n",
      "  (0, 8)\t-1.0\n",
      "  (0, 13)\t1.0\n",
      "  (0, 14)\t0.0\n"
     ]
    }
   ],
   "source": [
    "print(X[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4 unique tokens, one with a count of 1, some -1, notice that the position ranges from 0 to 15."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same Results With CountVectorizer.\n",
    "Now we can achieve the same results with CountVectorizer.\n",
    "\n",
    "Generate Raw Term Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 9)\n"
     ]
    }
   ],
   "source": [
    "print(X1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means there are 4 documents, and 9 unique items or you can say tokens. Where as using hashing we allowed for there to be 16 unique items."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the first document and how it compares to when we looked at it using hashing versus just counting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Compressed Sparse Row sparse matrix of dtype 'int64'\n",
      "\twith 5 stored elements and shape (1, 9)>\n",
      "  Coords\tValues\n",
      "  (0, 8)\t1\n",
      "  (0, 3)\t1\n",
      "  (0, 6)\t1\n",
      "  (0, 2)\t1\n",
      "  (0, 1)\t1\n"
     ]
    }
   ],
   "source": [
    "print(X1[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 5 items in the first document of the corpus. in the vector containing 8 unique items, shown below, there is one item which is sitting at positiion (0,8) in this document, there is 1 item thats sitting in position `(0,3)`.<br>\n",
    "\n",
    "For example, the **This** of the 1st documents **This is the first document** is at position `0`. Moreover, **This** is at position `6` in `vectorizer.get_feature_names_out()`. Therefore, **This** is `(0,6)` with the frequency of `1`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 13)\n"
     ]
    }
   ],
   "source": [
    "print(X2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There're 4 documents and 13 unique items/tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **3.3 Lower Casing** <a class=\"anchor\" id=\"3.3\"></a>\n",
    "\n",
    "\n",
    "[Table of Contents](#0.1)\n",
    "\n",
    "\n",
    "- Another pre-processing step which we will do is to transform our tweets into lower case. \n",
    "- This avoids having multiple copies of the same words. \n",
    "- For example, while calculating the word count, ‘Analytics’ and ‘analytics’ will be taken as different words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lower_case(df):\n",
    "    df['tweet'] = df['tweet'].apply(lambda x: \" \".join(x.lower() for x in x.split()))\n",
    "    print(df['tweet'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    @user when a father is dysfunctional and is so...\n",
      "1    @user @user thanks for #lyft credit i can't us...\n",
      "2                                  bihday your majesty\n",
      "3    #model i love u take with u all the time in ur...\n",
      "4                  factsguide: society now #motivation\n",
      "Name: tweet, dtype: object\n"
     ]
    }
   ],
   "source": [
    "lower_case(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    #studiolife #aislife #requires #passion #dedic...\n",
      "1    @user #white #supremacists want everyone to se...\n",
      "2    safe ways to heal your #acne!! #altwaystoheal ...\n",
      "3    is the hp and the cursed child book up for res...\n",
      "4    3rd #bihday to my amazing, hilarious #nephew e...\n",
      "Name: tweet, dtype: object\n"
     ]
    }
   ],
   "source": [
    "lower_case(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3.4 Punctuation Removal** <a class=\"anchor\" id=\"3.4\"></a>\n",
    "\n",
    "\n",
    "[Table of Contents](#0.1)\n",
    "\n",
    "\n",
    "- The next step is to remove punctuation as it doesn’t add any extra information while treating text data. \n",
    "\n",
    "- Therefore removing all instances of it will help us reduce the size of the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:2: SyntaxWarning: invalid escape sequence '\\w'\n",
      "<>:2: SyntaxWarning: invalid escape sequence '\\w'\n",
      "/var/folders/nh/cfqgdp9s0l396qrmcg1jl29c0000gn/T/ipykernel_6891/1163312214.py:2: SyntaxWarning: invalid escape sequence '\\w'\n",
      "  df['tweet'] = df['tweet'].str.replace('[^\\w\\s]','')\n"
     ]
    }
   ],
   "source": [
    "def punctuation_removal(df):\n",
    "    df['tweet'] = df['tweet'].str.replace('[^\\w\\s]','')\n",
    "    print(df['tweet'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    @user when a father is dysfunctional and is so...\n",
      "1    @user @user thanks for #lyft credit i can't us...\n",
      "2                                  bihday your majesty\n",
      "3    #model i love u take with u all the time in ur...\n",
      "4                  factsguide: society now #motivation\n",
      "Name: tweet, dtype: object\n"
     ]
    }
   ],
   "source": [
    "punctuation_removal(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    #studiolife #aislife #requires #passion #dedic...\n",
      "1    @user #white #supremacists want everyone to se...\n",
      "2    safe ways to heal your #acne!! #altwaystoheal ...\n",
      "3    is the hp and the cursed child book up for res...\n",
      "4    3rd #bihday to my amazing, hilarious #nephew e...\n",
      "Name: tweet, dtype: object\n"
     ]
    }
   ],
   "source": [
    "punctuation_removal(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can see that all the punctuation, including ‘#’ and ‘@’, has been removed from the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3.5 Stop Words Removal**  <a class=\"anchor\" id=\"3.5\"></a>\n",
    "\n",
    "\n",
    "[Table of Contents](#0.1)\n",
    "\n",
    "\n",
    "- As we discussed earlier, stop words (or commonly occurring words) should be removed from the text data. \n",
    "- For this purpose, we can either create a list of stopwords ourselves or we can use predefined libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stop_words_removal(df):\n",
    "    df['tweet'] = df['tweet'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
    "    print(df['tweet'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    @user father dysfunctional selfish drags kids ...\n",
      "1    @user @user thanks #lyft credit can't use caus...\n",
      "2                                       bihday majesty\n",
      "3    #model love u take u time urð±!!! ððð...\n",
      "4                      factsguide: society #motivation\n",
      "Name: tweet, dtype: object\n"
     ]
    }
   ],
   "source": [
    "stop_words_removal(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    #studiolife #aislife #requires #passion #dedic...\n",
      "1    @user #white #supremacists want everyone see n...\n",
      "2    safe ways heal #acne!! #altwaystoheal #healthy...\n",
      "3    hp cursed child book reservations already? yes...\n",
      "4    3rd #bihday amazing, hilarious #nephew eli ahm...\n",
      "Name: tweet, dtype: object\n"
     ]
    }
   ],
   "source": [
    "stop_words_removal(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3.6 Frequent Words Removal**  <a class=\"anchor\" id=\"3.6\"></a>\n",
    "\n",
    "\n",
    "[Table of Contents](#0.1)\n",
    "\n",
    "\n",
    "- We can also remove commonly occurring words from our text data.\n",
    "\n",
    "- First, let’s check the 10 most frequently occurring words in our text data then take call to remove or retain.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "@user    17291\n",
       "&amp;     1574\n",
       "day       1454\n",
       "#love     1449\n",
       "happy     1328\n",
       "-         1244\n",
       "u         1116\n",
       "love      1112\n",
       "i'm        992\n",
       "like       920\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq = pd.Series(' '.join(train['tweet']).split()).value_counts()[:10]\n",
    "freq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will remove these words as their presence will not be of any use in classification of our text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = list(freq.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frequent_words_removal(df):    \n",
    "    df['tweet'] = df['tweet'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq))\n",
    "    print(df['tweet'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    father dysfunctional selfish drags kids dysfun...\n",
      "1    thanks #lyft credit can't use cause offer whee...\n",
      "2                                       bihday majesty\n",
      "3    #model take time urð±!!! ðððð ð...\n",
      "4                      factsguide: society #motivation\n",
      "Name: tweet, dtype: object\n"
     ]
    }
   ],
   "source": [
    "frequent_words_removal(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    #studiolife #aislife #requires #passion #dedic...\n",
      "1    #white #supremacists want everyone see new â...\n",
      "2    safe ways heal #acne!! #altwaystoheal #healthy...\n",
      "3    hp cursed child book reservations already? yes...\n",
      "4    3rd #bihday amazing, hilarious #nephew eli ahm...\n",
      "Name: tweet, dtype: object\n"
     ]
    }
   ],
   "source": [
    "frequent_words_removal(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3.7 Rare Words Removal**  <a class=\"anchor\" id=\"3.7\"></a>\n",
    "\n",
    "\n",
    "[Table of Contents](#0.1)\n",
    "\n",
    "\n",
    "- Now, we will remove rarely occurring words from the text. \n",
    "- Because they’re so rare, the association between them and other words is dominated by noise. \n",
    "- We can replace rare words with a more general form and then this will have higher counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "anybody?               1\n",
       "....god                1\n",
       "#photographystudent    1\n",
       "#ucs                   1\n",
       "#kamp                  1\n",
       "#ucsdâ¦               1\n",
       "(even                  1\n",
       "deserve)               1\n",
       "hateful?               1\n",
       "omfg                   1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq = pd.Series(' '.join(train['tweet']).split()).value_counts()[-10:]\n",
    "freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = list(freq.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rare_words_removal(df):\n",
    "    df['tweet'] = df['tweet'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq))\n",
    "    print(df['tweet'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    father dysfunctional selfish drags kids dysfun...\n",
      "1    thanks #lyft credit can't use cause offer whee...\n",
      "2                                       bihday majesty\n",
      "3    #model take time urð±!!! ðððð ð...\n",
      "4                      factsguide: society #motivation\n",
      "Name: tweet, dtype: object\n"
     ]
    }
   ],
   "source": [
    "rare_words_removal(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    #studiolife #aislife #requires #passion #dedic...\n",
      "1    #white #supremacists want everyone see new â...\n",
      "2    safe ways heal #acne!! #altwaystoheal #healthy...\n",
      "3    hp cursed child book reservations already? yes...\n",
      "4    3rd #bihday amazing, hilarious #nephew eli ahm...\n",
      "Name: tweet, dtype: object\n"
     ]
    }
   ],
   "source": [
    "rare_words_removal(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- All these pre-processing steps are essential and help us in reducing our vocabulary clutter so that the features produced in the end are more effective."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3.8 Spelling Correction**  <a class=\"anchor\" id=\"3.8\"></a>\n",
    "\n",
    "\n",
    "[Table of Contents](#0.1)\n",
    "\n",
    "\n",
    "\n",
    "- Now tweets can be filled with plethora of spelling mistakes. Our task is to rectify these spelling mistakes.\n",
    "\n",
    "- In that context, spelling correction is a useful pre-processing step because this also will help us in reducing multiple copies of words. For example, “Analytics” and “analytcs” will be treated as different words even if they are used in the same sense.\n",
    "\n",
    "- To accomplish the above task, we will use the textblob library as follows-\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting TextBlob\n",
      "  Using cached textblob-0.18.0.post0-py3-none-any.whl.metadata (4.5 kB)\n",
      "Requirement already satisfied: nltk>=3.8 in /opt/anaconda3/envs/DataScienceStudy/lib/python3.12/site-packages (from TextBlob) (3.9.1)\n",
      "Requirement already satisfied: click in /opt/anaconda3/envs/DataScienceStudy/lib/python3.12/site-packages (from nltk>=3.8->TextBlob) (8.1.7)\n",
      "Requirement already satisfied: joblib in /opt/anaconda3/envs/DataScienceStudy/lib/python3.12/site-packages (from nltk>=3.8->TextBlob) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/anaconda3/envs/DataScienceStudy/lib/python3.12/site-packages (from nltk>=3.8->TextBlob) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in /opt/anaconda3/envs/DataScienceStudy/lib/python3.12/site-packages (from nltk>=3.8->TextBlob) (4.67.1)\n",
      "Using cached textblob-0.18.0.post0-py3-none-any.whl (626 kB)\n",
      "Installing collected packages: TextBlob\n",
      "Successfully installed TextBlob-0.18.0.post0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spell_correction(df):\n",
    "    return df['tweet'][:5].apply(lambda x: str(TextBlob(x).correct()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    father dysfunctional selfish drags kiss dysfun...\n",
       "1    thanks #left credit can't use cause offer whee...\n",
       "2                                       midday majesty\n",
       "3    #model take time or±!!! ðððð ð¦...\n",
       "4                      factsguide: society #motivation\n",
       "Name: tweet, dtype: object"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spell_correction(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    #studiolife #dislike #requires #passion #educa...\n",
       "1    #white #supremacists want everyone see new â...\n",
       "2    safe ways heal #acne!! #altwaystoheal #healthy...\n",
       "3    he cursed child book reservations already? yes...\n",
       "4    rd #midday amazing, hilarious #nephew epi thei...\n",
       "Name: tweet, dtype: object"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spell_correction(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3.9 Tokenization** <a class=\"anchor\" id=\"3.9\"></a>\n",
    "\n",
    "\n",
    "[Table of Contents](#0.1)\n",
    "\n",
    "\n",
    "- [Tokenization](https://www.geeksforgeeks.org/nlp-how-tokenizing-text-sentence-words-works/) refers to dividing the text into a sequence of words or sentences. \n",
    "\n",
    "- In our example, we have used the textblob library to first transform our tweets into a blob and then converted them into a series of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/thaophamsmacbook/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt_tab') # MUST HAVE THIS LINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokens(df):\n",
    "    return TextBlob(df['tweet'][1]).words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WordList(['thanks', 'lyft', 'credit', 'ca', \"n't\", 'use', 'cause', 'offer', 'wheelchair', 'vans', 'pdx', 'disapointed', 'getthanked'])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WordList(['white', 'supremacists', 'want', 'everyone', 'see', 'new', 'â\\x80\\x98', 'birdsâ\\x80\\x99', 'movie', 'â\\x80\\x94', 'hereâ\\x80\\x99s'])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3.10 Stemming** <a class=\"anchor\" id=\"3.10\"></a>\n",
    "\n",
    "\n",
    "[Table of Contents](#0.1)\n",
    "\n",
    "\n",
    "- [Stemming](https://www.geeksforgeeks.org/introduction-to-stemming/) refers to the removal of suffices, like “ing”, “ly”, “s”, etc. by a simple rule-based approach. \n",
    "\n",
    "- So, stemming takes a word and refers it back to its base or root form. **Stems**, **Stemming**, **Stemmed** and **Stemtization** are all based on the single word **stem**.\n",
    "\n",
    "- For this purpose, we will use *PorterStemmer* from the NLTK library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "st = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemming(df):\n",
    "    return df['tweet'][:5].apply(lambda x: \" \".join([st.stem(word) for word in x.split()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    father dysfunct selfish drag kid dysfunction. ...\n",
       "1    thank #lyft credit can't use caus offer wheelc...\n",
       "2                                       bihday majesti\n",
       "3    #model take time urð±!!! ðððð ð...\n",
       "4                           factsguide: societi #motiv\n",
       "Name: tweet, dtype: object"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemming(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    #studiolif #aislif #requir #passion #dedic #wi...\n",
       "1    #white #supremacist want everyon see new â #...\n",
       "2    safe way heal #acne!! #altwaystoh #healthi #he...\n",
       "3    hp curs child book reserv already? yes, where?...\n",
       "4    3rd #bihday amazing, hilari #nephew eli ahmir!...\n",
       "Name: tweet, dtype: object"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemming(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can see that *dysfunctional* has been transformed into *dysfunct*, among other changes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3.11 Lemmatization** <a class=\"anchor\" id=\"3.11\"></a>\n",
    "\n",
    "\n",
    "[Table of Contents](#0.1)\n",
    "\n",
    "\n",
    "- [Lemmatization](https://www.geeksforgeeks.org/python-lemmatization-with-nltk/) is the process of converting a word to its base form. The difference between stemming and lemmatization is, lemmatization considers the context and converts the word to its meaningful base form, whereas stemming just removes the last few characters, often leading to incorrect meanings and spelling errors.\n",
    "\n",
    "- Lemmatization is a more effective option than stemming because it converts the word into its root word, rather than just stripping the suffices. \n",
    "\n",
    "- Lemmatization makes use of the vocabulary and does a morphological analysis to obtain the root word. Therefore, we usually prefer using lemmatization over stemming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import Word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/thaophamsmacbook/nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet') # using the download() of nltk to specifically download the wordnet corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatization(df):\n",
    "    df['tweet'] = df['tweet'].apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()]))\n",
    "    print(df['tweet'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    father dysfunctional selfish drag kid dysfunct...\n",
      "1    thanks #lyft credit can't use cause offer whee...\n",
      "2                                       bihday majesty\n",
      "3    #model take time urð±!!! ðððð ð...\n",
      "4                      factsguide: society #motivation\n",
      "Name: tweet, dtype: object\n"
     ]
    }
   ],
   "source": [
    "lemmatization(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    #studiolife #aislife #requires #passion #dedic...\n",
      "1    #white #supremacists want everyone see new â...\n",
      "2    safe way heal #acne!! #altwaystoheal #healthy ...\n",
      "3    hp cursed child book reservation already? yes,...\n",
      "4    3rd #bihday amazing, hilarious #nephew eli ahm...\n",
      "Name: tweet, dtype: object\n"
     ]
    }
   ],
   "source": [
    "lemmatization(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **4. Advanced Text Processing** <a class=\"anchor\" id=\"4\"></a>\n",
    "\n",
    "\n",
    "[Table of Contents](#0.1)\n",
    "\n",
    "\n",
    "- Until now, we have covered all the basic pre-processing steps which are helpful in order to clean our data.\n",
    "\n",
    "- Now,we will move on and focus on advanced text-processing or NLP techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4.1 N-grams** <a class=\"anchor\" id=\"4.1\"></a>\n",
    "\n",
    "\n",
    "[Table of Contents](#0.1)\n",
    "\n",
    "\n",
    "- [N-grams](https://kavita-ganesan.com/what-are-n-grams/#.Xo3jccgzbIU) are the combination of multiple words used together. Ngrams with N=1 are called **unigrams**. Similarly, **bigrams (N=2)**, **trigrams (N=3)** and so on.\n",
    "\n",
    "- **Unigrams** do not usually contain as much information as compared to **bigrams** and **trigrams**. The basic principle behind n-grams is that they capture the language structure, like what letter or word is likely to follow the given one. \n",
    "\n",
    "- The longer the n-gram (the higher the n), the more context you have to work with. Optimum length really depends on the application – if your n-grams are too short, you may fail to capture important differences. On the other hand, if they are too long, you may fail to capture the “general knowledge” and only stick to particular cases.\n",
    "\n",
    "- Now, we will extract bigrams from our tweets using the ngrams function of the textblob library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combination_of_words(df):\n",
    "    return (TextBlob(df['tweet'][0]).ngrams(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[WordList(['father', 'dysfunctional']),\n",
       " WordList(['dysfunctional', 'selfish']),\n",
       " WordList(['selfish', 'drag']),\n",
       " WordList(['drag', 'kid']),\n",
       " WordList(['kid', 'dysfunction']),\n",
       " WordList(['dysfunction', 'run'])]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combination_of_words(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[WordList(['studiolife', 'aislife']),\n",
       " WordList(['aislife', 'requires']),\n",
       " WordList(['requires', 'passion']),\n",
       " WordList(['passion', 'dedication']),\n",
       " WordList(['dedication', 'willpower']),\n",
       " WordList(['willpower', 'find']),\n",
       " WordList(['find', 'newmaterialsâ\\x80¦'])]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combination_of_words(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4.2 Term Frequency**  <a class=\"anchor\" id=\"4.2\"></a>\n",
    "\n",
    "\n",
    "[Table of Contents](#0.1)\n",
    "\n",
    "\n",
    "- Term frequency is simply the ratio of the count of a word present in a sentence, to the length of the sentence.\n",
    "\n",
    "- Therefore, we can generalize term frequency as:\n",
    "\n",
    "**TF = (Number of times term T appears in the particular row) / (number of terms in that row)**\n",
    "\n",
    "\n",
    "- We will create a Term-Frequency table of a tweet as follows-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def term_frequency(df):\n",
    "    ## the pd.value_counts() is deprecated, therefore; we will use the pd.Series like below\n",
    "    # tf1 = (df['tweet'][1:2]).apply(lambda x: pd.value_counts(x.split(\" \"))).sum(axis = 0).reset_index() \n",
    "    # tf1.columns = ['words','tf']\n",
    "    # return tf1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def term_frequency(df):\n",
    "    tf1 = (df['tweet'][1:2]).apply(lambda x: pd.Series(x.split(\" \")).value_counts()).sum(axis=0).reset_index()\n",
    "    tf1.columns = ['words','tf']\n",
    "    return tf1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "      <th>tf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>thanks</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>#lyft</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>credit</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>can't</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>use</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    words  tf\n",
       "0  thanks   1\n",
       "1   #lyft   1\n",
       "2  credit   1\n",
       "3   can't   1\n",
       "4     use   1"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "term_frequency(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "      <th>tf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>#white</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>#supremacists</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>want</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>everyone</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>see</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           words  tf\n",
       "0         #white   1\n",
       "1  #supremacists   1\n",
       "2           want   1\n",
       "3       everyone   1\n",
       "4            see   1"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "term_frequency(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4.3 Inverse Document Frequency (IDF)** <a class=\"anchor\" id=\"4.3\"></a>\n",
    "\n",
    "\n",
    "[Table of Contents](#0.1)\n",
    "\n",
    "\n",
    "- The intuition behind inverse document frequency (IDF) is that a word is not of much use to us if it’s appearing in all the documents.\n",
    "\n",
    "- Therefore, the IDF of each word is the log of the ratio of the total number of rows to the number of rows in which that word is present.\n",
    "\n",
    "- IDF can be calculated as follows -\n",
    "\n",
    "   **IDF = log(N/n)**, \n",
    "   \n",
    " where, N is the total number of rows and n is the number of rows in which the word was present.\n",
    "\n",
    "- Now, we will calculate IDF for the same tweets for which we calculated the term frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf1 = (train['tweet'][1:2]).apply(lambda x: pd.value_counts(x.split(\" \"))).sum(axis = 0).reset_index()\n",
    "# tf1.columns = ['words','tf']\n",
    "# tf1.head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "      <th>tf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>thanks</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>#lyft</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>credit</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>can't</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>use</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    words  tf\n",
       "0  thanks   1\n",
       "1   #lyft   1\n",
       "2  credit   1\n",
       "3   can't   1\n",
       "4     use   1"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf1 = (train['tweet'][1:2]).apply(lambda x: pd.Series(x.split(\" \")).value_counts()).sum(axis=0).reset_index()\n",
    "tf1.columns = ['words','tf']\n",
    "tf1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "      <th>tf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>#white</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>#supremacists</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>want</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>everyone</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>see</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           words  tf\n",
       "0         #white   1\n",
       "1  #supremacists   1\n",
       "2           want   1\n",
       "3       everyone   1\n",
       "4            see   1"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf2 = (test['tweet'][1:2]).apply(lambda x: pd.Series(x.split(\" \")).value_counts()).sum(axis=0).reset_index()\n",
    "tf2.columns = ['words','tf']\n",
    "tf2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The more the value of IDF, the more unique is the word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4.4 Term Frequency – Inverse Document Frequency (TF-IDF)** <a class=\"anchor\" id=\"4.4\"></a>\n",
    "\n",
    "[Table of Contents](#0.1)\n",
    "\n",
    "\n",
    "- **TF-IDF** is the multiplication of the TF and IDF which we calculated again below for convinience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf1 = (train['tweet'][1:2]).apply(lambda x: pd.Series(x.split(\" \")).value_counts()).sum(axis=0).reset_index()\n",
    "tf1.columns = ['words','tf']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,word in enumerate(tf1['words']):\n",
    "    tf1.loc[i, 'idf'] = np.log(train.shape[0]/(len(train[train['tweet'].str.contains(word)])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "      <th>tf</th>\n",
       "      <th>idf</th>\n",
       "      <th>tfidf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>thanks</td>\n",
       "      <td>1</td>\n",
       "      <td>4.597751</td>\n",
       "      <td>4.597751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>#lyft</td>\n",
       "      <td>1</td>\n",
       "      <td>9.273691</td>\n",
       "      <td>9.273691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>credit</td>\n",
       "      <td>1</td>\n",
       "      <td>7.327781</td>\n",
       "      <td>7.327781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>can't</td>\n",
       "      <td>1</td>\n",
       "      <td>3.753564</td>\n",
       "      <td>3.753564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>use</td>\n",
       "      <td>1</td>\n",
       "      <td>3.382968</td>\n",
       "      <td>3.382968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>cause</td>\n",
       "      <td>1</td>\n",
       "      <td>5.610129</td>\n",
       "      <td>5.610129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>offer</td>\n",
       "      <td>1</td>\n",
       "      <td>6.522155</td>\n",
       "      <td>6.522155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>wheelchair</td>\n",
       "      <td>1</td>\n",
       "      <td>9.273691</td>\n",
       "      <td>9.273691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>van</td>\n",
       "      <td>1</td>\n",
       "      <td>5.236505</td>\n",
       "      <td>5.236505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>pdx.</td>\n",
       "      <td>1</td>\n",
       "      <td>8.762865</td>\n",
       "      <td>8.762865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>#disapointed</td>\n",
       "      <td>1</td>\n",
       "      <td>10.372303</td>\n",
       "      <td>10.372303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>#getthanked</td>\n",
       "      <td>1</td>\n",
       "      <td>9.679156</td>\n",
       "      <td>9.679156</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           words  tf        idf      tfidf\n",
       "0         thanks   1   4.597751   4.597751\n",
       "1          #lyft   1   9.273691   9.273691\n",
       "2         credit   1   7.327781   7.327781\n",
       "3          can't   1   3.753564   3.753564\n",
       "4            use   1   3.382968   3.382968\n",
       "5          cause   1   5.610129   5.610129\n",
       "6          offer   1   6.522155   6.522155\n",
       "7     wheelchair   1   9.273691   9.273691\n",
       "8            van   1   5.236505   5.236505\n",
       "9           pdx.   1   8.762865   8.762865\n",
       "10  #disapointed   1  10.372303  10.372303\n",
       "11   #getthanked   1   9.679156   9.679156"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf1['tfidf'] = tf1['tf'] * tf1['idf']\n",
    "tf1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can see that the TF-IDF has penalized words like ‘can’t’, and ‘use’ because they are commonly occurring words. However, it has given a high weight to “disappointed” since that will be very useful in determining the sentiment of the tweet.\n",
    "\n",
    "- We don’t have to calculate TF and IDF every time beforehand and then multiply it to obtain TF-IDF. Instead, sklearn has a separate function to directly obtain it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Compressed Sparse Row sparse matrix of dtype 'float64'\n",
       "\twith 112979 stored elements and shape (31962, 1000)>"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf = TfidfVectorizer(max_features=1000, lowercase=True, analyzer='word',\n",
    " stop_words= 'english',ngram_range=(1,1))\n",
    "train_vect = tfidf.fit_transform(train['tweet'])\n",
    "train_vect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4.5 Bag of Words** <a class=\"anchor\" id=\"4.5\"></a>\n",
    "\n",
    "[Table of Contents](#0.1)\n",
    "\n",
    "\n",
    "- [Bag of Words (BoW)](https://machinelearningmastery.com/gentle-introduction-bag-words-model/) refers to the representation of text which describes the presence of words within the text data. The intuition behind this is that two similar text fields will contain similar kind of words, and will therefore have a similar bag of words. Further, that from the text alone we can learn something about the meaning of the document.\n",
    "\n",
    "- For implementation, sklearn provides a separate function for it as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Compressed Sparse Row sparse matrix of dtype 'int64'\n",
       "\twith 132742 stored elements and shape (31962, 1000)>"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "bow = CountVectorizer(max_features=1000, lowercase=True, ngram_range=(1,1),analyzer = \"word\")\n",
    "train_bow = bow.fit_transform(train['tweet'])\n",
    "train_bow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4.6 Sentiment Analysis**  <a class=\"anchor\" id=\"4.6\"></a>\n",
    "\n",
    "[Table of Contents](#0.1)\n",
    "\n",
    "\n",
    "- Now we come to our problem which was to detect the sentiment of the tweet. So, before applying any ML/DL models (which can have a separate feature detecting the sentiment using the textblob library).\n",
    "\n",
    "- We will check the sentiment of the first few tweets as follows -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def polarity_subjectivity(df):\n",
    "    return df['tweet'][:5].apply(lambda x: TextBlob(x).sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    (-0.3, 0.5354166666666667)\n",
       "1                    (0.2, 0.2)\n",
       "2                    (0.0, 0.0)\n",
       "3                    (0.0, 0.0)\n",
       "4                    (0.0, 0.0)\n",
       "Name: tweet, dtype: object"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "polarity_subjectivity(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                    (0.0, 0.0)\n",
       "1    (0.06818181818181818, 0.22727272727272727)\n",
       "2                                (0.78125, 0.5)\n",
       "3                                    (0.5, 1.0)\n",
       "4                              (0.43125, 0.625)\n",
       "Name: tweet, dtype: object"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "polarity_subjectivity(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can can see that it returns a tuple representing polarity and subjectivity of each tweet. Here, we only extract polarity as it indicates the sentiment as value nearer to 1 means a positive sentiment and values nearer to -1 means a negative sentiment. This can also work as a feature for building a machine learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_analysis(df):\n",
    "    df['sentiment'] = df['tweet'].apply(lambda x: TextBlob(x).sentiment[0] )\n",
    "    return df[['tweet','sentiment']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>father dysfunctional selfish drag kid dysfunct...</td>\n",
       "      <td>-0.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>thanks #lyft credit can't use cause offer whee...</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bihday majesty</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>#model take time urð±!!! ðððð ð...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>factsguide: society #motivation</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet  sentiment\n",
       "0  father dysfunctional selfish drag kid dysfunct...       -0.3\n",
       "1  thanks #lyft credit can't use cause offer whee...        0.2\n",
       "2                                     bihday majesty        0.0\n",
       "3  #model take time urð±!!! ðððð ð...        0.0\n",
       "4                    factsguide: society #motivation        0.0"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_analysis(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>#studiolife #aislife #requires #passion #dedic...</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>#white #supremacists want everyone see new â...</td>\n",
       "      <td>0.068182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>safe way heal #acne!! #altwaystoheal #healthy ...</td>\n",
       "      <td>0.781250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hp cursed child book reservation already? yes,...</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3rd #bihday amazing, hilarious #nephew eli ahm...</td>\n",
       "      <td>0.431250</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet  sentiment\n",
       "0  #studiolife #aislife #requires #passion #dedic...   0.000000\n",
       "1  #white #supremacists want everyone see new â...   0.068182\n",
       "2  safe way heal #acne!! #altwaystoheal #healthy ...   0.781250\n",
       "3  hp cursed child book reservation already? yes,...   0.500000\n",
       "4  3rd #bihday amazing, hilarious #nephew eli ahm...   0.431250"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_analysis(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Go to Top](#0)\t"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 100982,
     "sourceId": 239192,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 29867,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "DataScienceStudy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
